"""
í†µí•© TOEFL ìŠ¤í”¼í‚¹ í‰ê°€ ì‹œìŠ¤í…œ
ìŒì„± ë¶„ì„ ëª¨ë¸ + LLM ê²°í•©
"""

import numpy as np
import json
from pathlib import Path
from typing import Dict, Optional

# ìŒì„± ì²˜ë¦¬
from audio_feature_extraction import AudioFeatureExtractor
from train_audio_model import predict_audio_scores

# LLM (MLX or OpenAI)
try:
    from mlx_lm import load, generate
    MLX_AVAILABLE = True
except ImportError:
    MLX_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False


class IntegratedTOEFLEvaluator:
    """
    ìŒì„± ë¶„ì„ + LLM í†µí•© í‰ê°€ ì‹œìŠ¤í…œ
    """

    def __init__(
        self,
        audio_model_dir: str = "./audio_model",
        llm_type: str = "mlx",  # "mlx" or "openai"
        llm_model: str = "mlx-community/Mistral-7B-Instruct-v0.2-4bit",
        llm_adapter_path: Optional[str] = "./toefl_finetuned_mlx",
        openai_api_key: Optional[str] = None
    ):
        """
        Args:
            audio_model_dir: ìŒì„± ë¶„ì„ ëª¨ë¸ ë””ë ‰í† ë¦¬
            llm_type: LLM ì¢…ë¥˜ ("mlx" or "openai")
            llm_model: LLM ëª¨ë¸ ì´ë¦„
            llm_adapter_path: MLX ì–´ëŒ‘í„° ê²½ë¡œ
            openai_api_key: OpenAI API í‚¤
        """

        # ìŒì„± íŠ¹ì§• ì¶”ì¶œê¸°
        self.audio_extractor = AudioFeatureExtractor()
        self.audio_model_dir = audio_model_dir

        # LLM ì„¤ì •
        self.llm_type = llm_type

        if llm_type == "mlx":
            if not MLX_AVAILABLE:
                raise ImportError("MLXê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install mlx mlx-lm")

            print(f"ğŸ“¥ MLX ëª¨ë¸ ë¡œë”©: {llm_model}")
            if llm_adapter_path and Path(llm_adapter_path).exists():
                self.model, self.tokenizer = load(llm_model, adapter_path=llm_adapter_path)
                print(f"   âœ… íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                self.model, self.tokenizer = load(llm_model)
                print(f"   âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        elif llm_type == "openai":
            if not OPENAI_AVAILABLE:
                raise ImportError("OpenAIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install openai")

            if openai_api_key:
                openai.api_key = openai_api_key
            self.llm_model = llm_model
            print(f"âœ… OpenAI ëª¨ë¸ ì¤€ë¹„: {llm_model}")

        print("âœ… í†µí•© í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ\n")

    def evaluate_audio(self, audio_path: str) -> Dict:
        """
        ìŒì„± íŒŒì¼ í‰ê°€ (ë°œìŒ, ìœ ì°½ì„±)

        Returns:
            {
                'pronunciation': float,
                'fluency': float,
                'audio_features': dict
            }
        """

        print(f"ğŸ¤ ìŒì„± ë¶„ì„ ì¤‘: {Path(audio_path).name}")

        # ìŒì„± íŠ¹ì§• ì¶”ì¶œ
        audio_features = self.audio_extractor.extract_all_features(audio_path)

        # íŠ¹ì§• ë²¡í„° ìƒì„±
        feature_vector = []
        for key, value in audio_features.items():
            if isinstance(value, list):
                feature_vector.extend(value)
            elif isinstance(value, (int, float)):
                feature_vector.append(value)

        feature_array = np.array(feature_vector)

        # ìŒì„± ëª¨ë¸ë¡œ ì ìˆ˜ ì˜ˆì¸¡
        scores = predict_audio_scores(
            feature_array,
            model_dir=self.audio_model_dir
        )

        print(f"   ë°œìŒ: {scores['pronunciation']:.2f}/4.0")
        print(f"   ìœ ì°½ì„±: {scores['fluency']:.2f}/4.0")

        return {
            'pronunciation': scores['pronunciation'],
            'fluency': scores['fluency'],
            'audio_features': audio_features
        }

    def evaluate_content(
        self,
        transcript: str,
        audio_scores: Dict,
        max_tokens: int = 500
    ) -> str:
        """
        LLMìœ¼ë¡œ ë‚´ìš©/ë¬¸ë²• í‰ê°€

        Args:
            transcript: ë°œí™” í…ìŠ¤íŠ¸
            audio_scores: ìŒì„± í‰ê°€ ê²°ê³¼
            max_tokens: ìµœëŒ€ í† í° ìˆ˜

        Returns:
            í‰ê°€ ê²°ê³¼ í…ìŠ¤íŠ¸
        """

        print(f"ğŸ“ ë‚´ìš©/ë¬¸ë²• í‰ê°€ ì¤‘...")

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""<|system|>
ë‹¹ì‹ ì€ TOEFL ìŠ¤í”¼í‚¹ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ìŒì„± ë¶„ì„ ê²°ê³¼:
- ë°œìŒ ì ìˆ˜: {audio_scores['pronunciation']:.2f}/4.0
- ìœ ì°½ì„± ì ìˆ˜: {audio_scores['fluency']:.2f}/4.0

ìœ„ ìŒì„± ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬, ì•„ë˜ í•™ìƒì˜ ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ í‰ê°€í•˜ì„¸ìš”:
1. ë‚´ìš© (Content): ì§ˆë¬¸ ì í•©ì„±, ë…¼ë¦¬ì  êµ¬ì¡°, êµ¬ì²´ì  ì˜ˆì‹œ
2. ë¬¸ë²•/í‘œí˜„ (Grammar): ë¬¸ë²• ì •í™•ì„±, ì–´íœ˜ ë‹¤ì–‘ì„±

ê° í•­ëª©ì— ëŒ€í•œ êµ¬ì²´ì  í”¼ë“œë°±ê³¼ ì ìˆ˜(0-4ì )ë¥¼ ì œê³µí•˜ì„¸ìš”.
<|user|>
í•™ìƒì˜ ë‹µë³€:

{transcript}
<|assistant|>
"""

        # LLM ì‹¤í–‰
        if self.llm_type == "mlx":
            response = generate(
                self.model,
                self.tokenizer,
                prompt=prompt,
                max_tokens=max_tokens,
                temp=0.7
            )
        elif self.llm_type == "openai":
            response = openai.ChatCompletion.create(
                model=self.llm_model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ TOEFL í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )
            response = response.choices[0].message.content

        return response

    def evaluate_complete(
        self,
        audio_path: str,
        transcript: Optional[str] = None,
        use_whisper: bool = False
    ) -> Dict:
        """
        ì™„ì „ í†µí•© í‰ê°€ (ìŒì„± + í…ìŠ¤íŠ¸)

        Args:
            audio_path: ìŒì„± íŒŒì¼ ê²½ë¡œ
            transcript: ë°œí™” í…ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ Whisperë¡œ ìë™ ì¸ì‹)
            use_whisper: Whisperë¡œ ìë™ ì „ì‚¬ ì—¬ë¶€

        Returns:
            ì¢…í•© í‰ê°€ ê²°ê³¼
        """

        print("=" * 60)
        print("ğŸ“ TOEFL ìŠ¤í”¼í‚¹ í†µí•© í‰ê°€")
        print("=" * 60)
        print()

        # 1. ìŒì„± í‰ê°€
        audio_result = self.evaluate_audio(audio_path)
        print()

        # 2. í…ìŠ¤íŠ¸ ì¤€ë¹„
        if transcript is None and use_whisper:
            print("ğŸ™ï¸  Whisperë¡œ ìŒì„± ì¸ì‹ ì¤‘...")
            transcript = self.transcribe_with_whisper(audio_path)
            print(f"   ì¸ì‹ ê²°ê³¼: {transcript[:100]}...")
            print()
        elif transcript is None:
            raise ValueError("transcriptë¥¼ ì œê³µí•˜ê±°ë‚˜ use_whisper=Trueë¡œ ì„¤ì •í•˜ì„¸ìš”.")

        # 3. ë‚´ìš©/ë¬¸ë²• í‰ê°€
        content_result = self.evaluate_content(transcript, audio_result)
        print()

        # 4. ì¢…í•© ê²°ê³¼
        result = {
            'audio_file': Path(audio_path).name,
            'transcript': transcript,
            'pronunciation_score': audio_result['pronunciation'],
            'fluency_score': audio_result['fluency'],
            'content_evaluation': content_result,
            'audio_features': audio_result['audio_features']
        }

        print("=" * 60)
        print("ğŸ“Š í‰ê°€ ì™„ë£Œ")
        print("=" * 60)
        print(f"ë°œìŒ: {result['pronunciation_score']:.2f}/4.0")
        print(f"ìœ ì°½ì„±: {result['fluency_score']:.2f}/4.0")
        print()
        print("ìƒì„¸ í‰ê°€:")
        print(content_result)
        print("=" * 60)

        return result

    def transcribe_with_whisper(self, audio_path: str) -> str:
        """Whisperë¡œ ìŒì„± ì¸ì‹"""
        try:
            import whisper
        except ImportError:
            raise ImportError("Whisperê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install openai-whisper")

        model = whisper.load_model("base")
        result = model.transcribe(audio_path)
        return result["text"]

    def batch_evaluate(
        self,
        audio_dir: str,
        csv_path: str,
        output_path: str = "evaluation_results.jsonl"
    ):
        """ì—¬ëŸ¬ ìŒì„± íŒŒì¼ ì¼ê´„ í‰ê°€"""

        import pandas as pd

        df = pd.read_csv(csv_path)
        audio_files = list(Path(audio_dir).glob("*.wav"))

        results = []

        for i, audio_file in enumerate(audio_files):
            print(f"\n[{i+1}/{len(audio_files)}] í‰ê°€ ì¤‘...")

            # CSVì—ì„œ ëŒ€ë³¸ ì°¾ê¸°
            file_id = audio_file.stem
            matching_row = df[df['íŒŒì¼ ì´ë¦„'].str.contains(file_id, na=False)]

            if not matching_row.empty:
                transcript = matching_row.iloc[0].get('í…ìŠ¤íŠ¸', '')

                try:
                    result = self.evaluate_complete(
                        str(audio_file),
                        transcript=transcript
                    )
                    results.append(result)
                except Exception as e:
                    print(f"âŒ ì˜¤ë¥˜: {e}")
                    continue

        # ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            for result in results:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')

        print(f"\nâœ… ì¼ê´„ í‰ê°€ ì™„ë£Œ: {len(results)}ê°œ íŒŒì¼")
        print(f"ğŸ’¾ ì €ì¥: {output_path}")

        return results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='í†µí•© TOEFL í‰ê°€ ì‹œìŠ¤í…œ')
    parser.add_argument('--audio', type=str,
                        help='í‰ê°€í•  ìŒì„± íŒŒì¼')
    parser.add_argument('--transcript', type=str,
                        help='ë°œí™” í…ìŠ¤íŠ¸ (ì„ íƒ)')
    parser.add_argument('--audio_model', type=str, default='./audio_model',
                        help='ìŒì„± ëª¨ë¸ ë””ë ‰í† ë¦¬')
    parser.add_argument('--llm_type', type=str, default='mlx',
                        choices=['mlx', 'openai'],
                        help='LLM ì¢…ë¥˜')
    parser.add_argument('--llm_model', type=str,
                        default='mlx-community/Mistral-7B-Instruct-v0.2-4bit',
                        help='LLM ëª¨ë¸')
    parser.add_argument('--adapter', type=str, default='./toefl_finetuned_mlx',
                        help='MLX ì–´ëŒ‘í„° ê²½ë¡œ')
    parser.add_argument('--use_whisper', action='store_true',
                        help='Whisperë¡œ ìë™ ì „ì‚¬')

    # ì¼ê´„ í‰ê°€ ì˜µì…˜
    parser.add_argument('--batch', action='store_true',
                        help='ì¼ê´„ í‰ê°€ ëª¨ë“œ')
    parser.add_argument('--audio_dir', type=str,
                        help='ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬ (ì¼ê´„ í‰ê°€)')
    parser.add_argument('--csv', type=str,
                        help='CSV íŒŒì¼ (ì¼ê´„ í‰ê°€)')

    args = parser.parse_args()

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    evaluator = IntegratedTOEFLEvaluator(
        audio_model_dir=args.audio_model,
        llm_type=args.llm_type,
        llm_model=args.llm_model,
        llm_adapter_path=args.adapter
    )

    if args.batch:
        # ì¼ê´„ í‰ê°€
        if not args.audio_dir or not args.csv:
            print("âŒ --audio_dirì™€ --csvë¥¼ ì§€ì •í•˜ì„¸ìš”.")
        else:
            evaluator.batch_evaluate(args.audio_dir, args.csv)
    else:
        # ë‹¨ì¼ í‰ê°€
        if not args.audio:
            print("âŒ --audioë¥¼ ì§€ì •í•˜ì„¸ìš”.")
        else:
            result = evaluator.evaluate_complete(
                args.audio,
                transcript=args.transcript,
                use_whisper=args.use_whisper
            )

            # ê²°ê³¼ ì €ì¥
            output_file = f"result_{Path(args.audio).stem}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")

"""
ìŒì„± íŠ¹ì§• ì¶”ì¶œ ë° ë°œìŒ/ìœ ì°½ì„± ë¶„ì„
MFCC, Pitch, Energy ë“± ìŒì„± íŠ¹ì§• ì¶”ì¶œ
"""

import numpy as np
import librosa
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
import json
import warnings
warnings.filterwarnings('ignore')


class AudioFeatureExtractor:
    """ìŒì„± íŒŒì¼ì—ì„œ TOEFL í‰ê°€ì— í•„ìš”í•œ íŠ¹ì§• ì¶”ì¶œ"""

    def __init__(self, sample_rate: int = 16000):
        self.sample_rate = sample_rate

    def extract_mfcc_features(self, audio_path: str, n_mfcc: int = 13) -> Dict:
        """
        MFCC (Mel-frequency cepstral coefficients) ì¶”ì¶œ
        ë°œìŒ íŠ¹ì§• ë¶„ì„ì— ì‚¬ìš©
        """
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        y, sr = librosa.load(audio_path, sr=self.sample_rate)

        # MFCC ì¶”ì¶œ
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
        mfcc_delta = librosa.feature.delta(mfcc)
        mfcc_delta2 = librosa.feature.delta(mfcc, order=2)

        return {
            'mfcc_mean': np.mean(mfcc, axis=1).tolist(),
            'mfcc_std': np.std(mfcc, axis=1).tolist(),
            'mfcc_delta_mean': np.mean(mfcc_delta, axis=1).tolist(),
            'mfcc_delta2_mean': np.mean(mfcc_delta2, axis=1).tolist(),
        }

    def extract_prosody_features(self, audio_path: str) -> Dict:
        """
        ìš´ìœ¨(Prosody) íŠ¹ì§• ì¶”ì¶œ
        ìœ ì°½ì„±, ì–µì–‘, ë¦¬ë“¬ ë¶„ì„ì— ì‚¬ìš©
        """
        y, sr = librosa.load(audio_path, sr=self.sample_rate)

        # Pitch (F0) ì¶”ì¶œ
        pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
        pitch_values = []
        for t in range(pitches.shape[1]):
            index = magnitudes[:, t].argmax()
            pitch = pitches[index, t]
            if pitch > 0:
                pitch_values.append(pitch)

        # Energy (ìŒëŸ‰)
        energy = librosa.feature.rms(y=y)[0]

        # Zero Crossing Rate (ìŒì„±/ë¬´ìŒ êµ¬ë¶„)
        zcr = librosa.feature.zero_crossing_rate(y)[0]

        # Spectral features (ìŒì§ˆ)
        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]

        # ë§í•˜ê¸° ì†ë„ (ìŒì ˆ ìˆ˜ ì¶”ì •)
        onset_env = librosa.onset.onset_strength(y=y, sr=sr)
        tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)[0]

        return {
            'pitch_mean': np.mean(pitch_values) if pitch_values else 0,
            'pitch_std': np.std(pitch_values) if pitch_values else 0,
            'pitch_range': np.ptp(pitch_values) if pitch_values else 0,
            'energy_mean': float(np.mean(energy)),
            'energy_std': float(np.std(energy)),
            'zcr_mean': float(np.mean(zcr)),
            'spectral_centroid_mean': float(np.mean(spectral_centroid)),
            'spectral_rolloff_mean': float(np.mean(spectral_rolloff)),
            'tempo': float(tempo),
            'duration': float(librosa.get_duration(y=y, sr=sr))
        }

    def extract_pronunciation_features(self, audio_path: str) -> Dict:
        """
        ë°œìŒ ê´€ë ¨ íŠ¹ì§• ì¶”ì¶œ
        ììŒ/ëª¨ìŒ ëª…í™•ì„±, ìŒì†Œ ì •í™•ë„
        """
        y, sr = librosa.load(audio_path, sr=self.sample_rate)

        # Formants (ëª¨ìŒ ë¶„ì„)
        # F1, F2ë¡œ ëª¨ìŒ êµ¬ë¶„ ê°€ëŠ¥
        S = np.abs(librosa.stft(y))
        freqs = librosa.fft_frequencies(sr=sr)

        # Spectral contrast (ììŒ ëª…í™•ì„±)
        contrast = librosa.feature.spectral_contrast(y=y, sr=sr)

        # Chroma features (ìŒë†’ì´ ì •í™•ë„)
        chroma = librosa.feature.chroma_stft(y=y, sr=sr)

        return {
            'spectral_contrast_mean': np.mean(contrast, axis=1).tolist(),
            'chroma_mean': np.mean(chroma, axis=1).tolist(),
            'spectral_flux': float(np.mean(np.diff(S, axis=1)))
        }

    def extract_fluency_features(self, audio_path: str) -> Dict:
        """
        ìœ ì°½ì„± ê´€ë ¨ íŠ¹ì§• ì¶”ì¶œ
        íœ´ì§€(pause), ë§ë”ë“¬, ì†ë„ ë³€í™”
        """
        y, sr = librosa.load(audio_path, sr=self.sample_rate)

        # ë¬´ìŒ êµ¬ê°„ íƒì§€ (íœ´ì§€)
        intervals = librosa.effects.split(y, top_db=30)

        # íœ´ì§€ ì‹œê°„ ê³„ì‚°
        pauses = []
        for i in range(len(intervals) - 1):
            pause_start = intervals[i][1]
            pause_end = intervals[i + 1][0]
            pause_duration = (pause_end - pause_start) / sr
            pauses.append(pause_duration)

        # ë°œí™” êµ¬ê°„ ê¸¸ì´
        speech_durations = [(end - start) / sr for start, end in intervals]

        return {
            'num_pauses': len(pauses),
            'pause_mean': float(np.mean(pauses)) if pauses else 0,
            'pause_std': float(np.std(pauses)) if pauses else 0,
            'pause_total': float(np.sum(pauses)) if pauses else 0,
            'speech_rate': len(intervals) / (len(y) / sr),  # ë°œí™” êµ¬ê°„/ì´ˆ
            'speech_duration_mean': float(np.mean(speech_durations)) if speech_durations else 0,
            'articulation_rate': len(intervals) / (np.sum(speech_durations) if speech_durations else 1)
        }

    def extract_all_features(self, audio_path: str) -> Dict:
        """ëª¨ë“  ìŒì„± íŠ¹ì§• í•œë²ˆì— ì¶”ì¶œ"""
        features = {}

        features.update(self.extract_mfcc_features(audio_path))
        features.update(self.extract_prosody_features(audio_path))
        features.update(self.extract_pronunciation_features(audio_path))
        features.update(self.extract_fluency_features(audio_path))

        return features


def process_audio_dataset(
    audio_dir: str,
    csv_path: str,
    output_path: str = "audio_features.jsonl"
):
    """
    ì „ì²´ ë°ì´í„°ì…‹ì˜ ìŒì„± íŠ¹ì§• ì¶”ì¶œ

    Args:
        audio_dir: WAV íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        csv_path: í”¼ë“œë°± CSV íŒŒì¼
        output_path: ì¶œë ¥ JSONL íŒŒì¼
    """

    extractor = AudioFeatureExtractor()

    # CSV ë¡œë“œ
    df = pd.read_csv(csv_path)

    results = []

    print(f"ğŸ“‚ ìŒì„± íŒŒì¼ ì²˜ë¦¬ ì¤‘: {audio_dir}")
    print(f"ğŸ“Š CSV íŒŒì¼: {csv_path}")
    print()

    # ê° WAV íŒŒì¼ ì²˜ë¦¬
    audio_files = list(Path(audio_dir).glob("*.wav"))

    for i, audio_file in enumerate(audio_files):
        print(f"[{i+1}/{len(audio_files)}] ì²˜ë¦¬ ì¤‘: {audio_file.name}")

        try:
            # ìŒì„± íŠ¹ì§• ì¶”ì¶œ
            features = extractor.extract_all_features(str(audio_file))

            # íŒŒì¼ëª…ìœ¼ë¡œ CSVì—ì„œ ë§¤ì¹­ (íŒŒì¼ëª… ê·œì¹™ì— ë”°ë¼ ìˆ˜ì • í•„ìš”)
            file_id = audio_file.stem  # í™•ì¥ì ì œì™¸í•œ íŒŒì¼ëª…

            # CSVì—ì„œ í•´ë‹¹ í–‰ ì°¾ê¸°
            matching_row = df[df['íŒŒì¼ ì´ë¦„'].str.contains(file_id, na=False)]

            if not matching_row.empty:
                row = matching_row.iloc[0]

                result = {
                    'audio_file': audio_file.name,
                    'file_id': file_id,
                    'audio_features': features,
                    'ground_truth': {
                        'transcript': row.get('í…ìŠ¤íŠ¸', ''),
                        'pronunciation_score': row.get('ë°œìŒ', ''),
                        'fluency_score': row.get('fluency', ''),
                        'content_score': row.get('ë‚´ìš©', ''),
                        'grammar_score': row.get('ë¬¸ë²•/í‘œí˜„', ''),
                        'total_score': row.get('total_score', 0),
                        'feedback': row.get('í…ìŠ¤íŠ¸ í”¼ë“œë°±', '')
                    }
                }

                results.append(result)
            else:
                print(f"  âš ï¸  CSVì—ì„œ ë§¤ì¹­ë˜ëŠ” í–‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {file_id}")

        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")
            continue

    # JSONLë¡œ ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')

    print(f"\nâœ… ì™„ë£Œ! {len(results)}ê°œ íŒŒì¼ ì²˜ë¦¬")
    print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {output_path}")

    return results


def create_feature_summary(jsonl_path: str):
    """ì¶”ì¶œëœ íŠ¹ì§• ìš”ì•½ í†µê³„"""

    with open(jsonl_path, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]

    print(f"\nğŸ“Š ë°ì´í„°ì…‹ ìš”ì•½")
    print(f"ì´ ìƒ˜í”Œ ìˆ˜: {len(data)}")
    print()

    # ì ìˆ˜ ë¶„í¬
    scores = [d['ground_truth']['total_score'] for d in data]
    print(f"ì ìˆ˜ ë¶„í¬:")
    print(f"  í‰ê· : {np.mean(scores):.2f}")
    print(f"  í‘œì¤€í¸ì°¨: {np.std(scores):.2f}")
    print(f"  ë²”ìœ„: {np.min(scores):.1f} - {np.max(scores):.1f}")
    print()

    # ìŒì„± íŠ¹ì§• ìš”ì•½
    durations = [d['audio_features']['duration'] for d in data]
    tempos = [d['audio_features']['tempo'] for d in data]

    print(f"ìŒì„± íŠ¹ì§•:")
    print(f"  í‰ê·  ê¸¸ì´: {np.mean(durations):.1f}ì´ˆ")
    print(f"  í‰ê·  í…œí¬: {np.mean(tempos):.1f} BPM")
    print()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='TOEFL ìŒì„± íŠ¹ì§• ì¶”ì¶œ')
    parser.add_argument('--audio_dir', type=str, required=True,
                        help='WAV íŒŒì¼ ë””ë ‰í† ë¦¬')
    parser.add_argument('--csv', type=str, required=True,
                        help='í”¼ë“œë°± CSV íŒŒì¼')
    parser.add_argument('--output', type=str, default='audio_features.jsonl',
                        help='ì¶œë ¥ JSONL íŒŒì¼')

    args = parser.parse_args()

    # ì˜ˆì‹œ ì‹¤í–‰
    if Path(args.audio_dir).exists() and Path(args.csv).exists():
        results = process_audio_dataset(
            args.audio_dir,
            args.csv,
            args.output
        )

        create_feature_summary(args.output)
    else:
        print("âŒ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        print(f"ì˜ˆì‹œ: python audio_feature_extraction.py --audio_dir ./audio --csv feedback.csv")

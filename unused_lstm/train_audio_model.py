"""
ìŒì„± íŠ¹ì§• ê¸°ë°˜ ë°œìŒ/ìœ ì°½ì„± í‰ê°€ ëª¨ë¸
LSTMì„ ì‚¬ìš©í•œ íšŒê·€ ëª¨ë¸
"""

import numpy as np
import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pickle
from pathlib import Path
from typing import Dict, Tuple


class AudioFeaturesDataset(Dataset):
    """ìŒì„± íŠ¹ì§• ë°ì´í„°ì…‹"""

    def __init__(self, features, labels, scaler=None):
        self.features = features
        self.labels = labels

        # íŠ¹ì§• ì •ê·œí™”
        if scaler is None:
            self.scaler = StandardScaler()
            self.features = self.scaler.fit_transform(features)
        else:
            self.scaler = scaler
            self.features = self.scaler.transform(features)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return (
            torch.FloatTensor(self.features[idx]),
            torch.FloatTensor(self.labels[idx])
        )


class AudioEvaluationModel(nn.Module):
    """
    ìŒì„± íŠ¹ì§• ê¸°ë°˜ í‰ê°€ ëª¨ë¸
    ë°œìŒ, ìœ ì°½ì„± ì ìˆ˜ ì˜ˆì¸¡
    """

    def __init__(self, input_dim: int, hidden_dim: int = 128, num_layers: int = 2):
        super().__init__()

        # LSTM ë ˆì´ì–´
        self.lstm = nn.LSTM(
            input_dim,
            hidden_dim,
            num_layers,
            batch_first=True,
            dropout=0.3 if num_layers > 1 else 0
        )

        # Fully connected layers
        self.fc_layers = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 2)  # ë°œìŒ, ìœ ì°½ì„± ì ìˆ˜ (0-4)
        )

    def forward(self, x):
        # x shape: (batch, features)
        # LSTM ì…ë ¥ì„ ìœ„í•´ ì°¨ì› ì¶”ê°€: (batch, seq_len=1, features)
        x = x.unsqueeze(1)

        # LSTM
        lstm_out, (hidden, cell) = self.lstm(x)

        # ë§ˆì§€ë§‰ hidden state ì‚¬ìš©
        output = self.fc_layers(hidden[-1])

        return output


def prepare_dataset_from_jsonl(jsonl_path: str) -> Tuple:
    """
    JSONLì—ì„œ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬

    Returns:
        features: numpy array of audio features
        labels: numpy array of [pronunciation_score, fluency_score]
    """

    with open(jsonl_path, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]

    features_list = []
    labels_list = []

    for item in data:
        # ìŒì„± íŠ¹ì§• ì¶”ì¶œ (ìˆ«ìí˜•ë§Œ)
        audio_features = item['audio_features']

        feature_vector = []
        for key, value in audio_features.items():
            if isinstance(value, list):
                feature_vector.extend(value)
            elif isinstance(value, (int, float)):
                feature_vector.append(value)

        # ë ˆì´ë¸”: ë°œìŒ, ìœ ì°½ì„± ì ìˆ˜
        # í…ìŠ¤íŠ¸ ì ìˆ˜ë¥¼ ìˆ«ìë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
        try:
            pronunciation = float(item['ground_truth'].get('total_score', 0))
            fluency = float(item['ground_truth'].get('total_score', 0))
            # TODO: ì‹¤ì œë¡œëŠ” ë°œìŒ/ìœ ì°½ì„± ì ìˆ˜ê°€ ë³„ë„ë¡œ ìˆì–´ì•¼ í•¨
        except:
            continue

        features_list.append(feature_vector)
        labels_list.append([pronunciation, fluency])

    features = np.array(features_list)
    labels = np.array(labels_list)

    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"   íŠ¹ì§• ì°¨ì›: {features.shape}")
    print(f"   ë ˆì´ë¸” ì°¨ì›: {labels.shape}")

    return features, labels


def train_audio_model(
    jsonl_path: str,
    output_dir: str = "./audio_model",
    epochs: int = 100,
    batch_size: int = 32,
    learning_rate: float = 0.001,
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
):
    """
    ìŒì„± í‰ê°€ ëª¨ë¸ í•™ìŠµ

    Args:
        jsonl_path: ìŒì„± íŠ¹ì§• JSONL íŒŒì¼
        output_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        epochs: í•™ìŠµ ì—í¬í¬
        batch_size: ë°°ì¹˜ í¬ê¸°
        learning_rate: í•™ìŠµë¥ 
        device: cuda or cpu
    """

    Path(output_dir).mkdir(exist_ok=True)

    print("=" * 60)
    print("ìŒì„± í‰ê°€ ëª¨ë¸ í•™ìŠµ")
    print("=" * 60)
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    print()

    # ë°ì´í„° ë¡œë“œ
    features, labels = prepare_dataset_from_jsonl(jsonl_path)

    # Train/Test split
    X_train, X_test, y_train, y_test = train_test_split(
        features, labels, test_size=0.2, random_state=42
    )

    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = AudioFeaturesDataset(X_train, y_train)
    test_dataset = AudioFeaturesDataset(X_test, y_test, scaler=train_dataset.scaler)

    # DataLoader
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size)

    # ëª¨ë¸ ì´ˆê¸°í™”
    input_dim = features.shape[1]
    model = AudioEvaluationModel(input_dim=input_dim, hidden_dim=128, num_layers=2)
    model = model.to(device)

    # Loss & Optimizer
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=10
    )

    print(f"ëª¨ë¸ ì•„í‚¤í…ì²˜:")
    print(model)
    print()

    # í•™ìŠµ
    best_loss = float('inf')

    for epoch in range(epochs):
        # Train
        model.train()
        train_loss = 0.0

        for features_batch, labels_batch in train_loader:
            features_batch = features_batch.to(device)
            labels_batch = labels_batch.to(device)

            # Forward
            outputs = model(features_batch)
            loss = criterion(outputs, labels_batch)

            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        train_loss /= len(train_loader)

        # Validation
        model.eval()
        test_loss = 0.0

        with torch.no_grad():
            for features_batch, labels_batch in test_loader:
                features_batch = features_batch.to(device)
                labels_batch = labels_batch.to(device)

                outputs = model(features_batch)
                loss = criterion(outputs, labels_batch)
                test_loss += loss.item()

        test_loss /= len(test_loader)

        # Learning rate scheduling
        scheduler.step(test_loss)

        # ì¶œë ¥
        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{epochs}] "
                  f"Train Loss: {train_loss:.4f} | "
                  f"Test Loss: {test_loss:.4f}")

        # ìµœê³  ëª¨ë¸ ì €ì¥
        if test_loss < best_loss:
            best_loss = test_loss
            torch.save(model.state_dict(), f"{output_dir}/best_model.pth")

    print()
    print(f"âœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"   ìµœê³  Test Loss: {best_loss:.4f}")

    # Scaler ì €ì¥
    with open(f"{output_dir}/scaler.pkl", 'wb') as f:
        pickle.dump(train_dataset.scaler, f)

    # ëª¨ë¸ ë©”íƒ€ì •ë³´ ì €ì¥
    metadata = {
        'input_dim': input_dim,
        'hidden_dim': 128,
        'num_layers': 2,
        'best_loss': best_loss
    }

    with open(f"{output_dir}/metadata.json", 'w') as f:
        json.dump(metadata, f, indent=2)

    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {output_dir}/")

    return model, train_dataset.scaler


def predict_audio_scores(
    audio_features: np.ndarray,
    model_dir: str = "./audio_model",
    device: str = "cpu"
) -> Dict:
    """
    í•™ìŠµëœ ëª¨ë¸ë¡œ ë°œìŒ/ìœ ì°½ì„± ì ìˆ˜ ì˜ˆì¸¡

    Args:
        audio_features: ìŒì„± íŠ¹ì§• ë²¡í„°
        model_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬
        device: cuda or cpu

    Returns:
        {'pronunciation': float, 'fluency': float}
    """

    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    with open(f"{model_dir}/metadata.json", 'r') as f:
        metadata = json.load(f)

    # ëª¨ë¸ ë¡œë“œ
    model = AudioEvaluationModel(
        input_dim=metadata['input_dim'],
        hidden_dim=metadata['hidden_dim'],
        num_layers=metadata['num_layers']
    )
    model.load_state_dict(torch.load(f"{model_dir}/best_model.pth", map_location=device))
    model = model.to(device)
    model.eval()

    # Scaler ë¡œë“œ
    with open(f"{model_dir}/scaler.pkl", 'rb') as f:
        scaler = pickle.load(f)

    # íŠ¹ì§• ì •ê·œí™”
    features_scaled = scaler.transform(audio_features.reshape(1, -1))
    features_tensor = torch.FloatTensor(features_scaled).to(device)

    # ì˜ˆì¸¡
    with torch.no_grad():
        scores = model(features_tensor)
        scores = scores.cpu().numpy()[0]

    return {
        'pronunciation': float(scores[0]),
        'fluency': float(scores[1])
    }


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='ìŒì„± í‰ê°€ ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--data', type=str, required=True,
                        help='ìŒì„± íŠ¹ì§• JSONL íŒŒì¼')
    parser.add_argument('--output', type=str, default='./audio_model',
                        help='ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--epochs', type=int, default=100,
                        help='í•™ìŠµ ì—í¬í¬')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--lr', type=float, default=0.001,
                        help='í•™ìŠµë¥ ')

    args = parser.parse_args()

    if Path(args.data).exists():
        train_audio_model(
            jsonl_path=args.data,
            output_dir=args.output,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.lr
        )
    else:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.data}")
        print("ë¨¼ì € audio_feature_extraction.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

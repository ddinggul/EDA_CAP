"""
HuggingFace λ¨λΈ νμΈνλ‹ μ¤ν¬λ¦½νΈ
Llama, Mistral, Gemma λ“±μ μ¤ν”μ†μ¤ λ¨λΈμ„ νμΈνλ‹
"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import json


def load_and_prepare_data(jsonl_path: str, tokenizer):
    """ν•™μµ λ°μ΄ν„° λ΅λ“ λ° μ „μ²λ¦¬"""
    print(f"π“‚ λ°μ΄ν„° λ΅λ”©: {jsonl_path}")

    # JSONL νμΌ λ΅λ“
    dataset = load_dataset('json', data_files=jsonl_path, split='train')

    def tokenize_function(examples):
        # ν…μ¤νΈλ¥Ό ν† ν°ν™”
        return tokenizer(
            examples['text'],
            truncation=True,
            max_length=1024,
            padding='max_length'
        )

    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names
    )

    return tokenized_dataset


def setup_lora_config():
    """LoRA μ„¤μ • (ν¨μ¨μ μΈ νμΈνλ‹)"""
    return LoraConfig(
        r=16,  # LoRA rank
        lora_alpha=32,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # Llama/Mistral κΈ°μ¤€
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )


def fine_tune_model(
    model_name: str = "meta-llama/Llama-2-7b-chat-hf",
    training_data_path: str = "training_data_huggingface.jsonl",
    output_dir: str = "./toefl_finetuned_model"
):
    """
    HuggingFace λ¨λΈ νμΈνλ‹

    Args:
        model_name: λ² μ΄μ¤ λ¨λΈ μ΄λ¦„
            - "meta-llama/Llama-2-7b-chat-hf" (κ¶μ¥)
            - "mistralai/Mistral-7B-Instruct-v0.2"
            - "google/gemma-7b-it"
        training_data_path: ν•™μµ λ°μ΄ν„° κ²½λ΅
        output_dir: λ¨λΈ μ €μ¥ κ²½λ΅
    """

    print("=" * 60)
    print(f"HuggingFace λ¨λΈ νμΈνλ‹: {model_name}")
    print("=" * 60)

    # 1. ν† ν¬λ‚μ΄μ € λ΅λ“
    print("\nπ“¥ ν† ν¬λ‚μ΄μ € λ΅λ”©...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    # 2. λ¨λΈ λ΅λ“ (4bit μ–‘μν™”λ΅ λ©”λ¨λ¦¬ μ μ•½)
    print("\nπ“¥ λ¨λΈ λ΅λ”©...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        load_in_4bit=True,  # 4bit μ–‘μν™” (λ©”λ¨λ¦¬ μ μ•½)
        torch_dtype=torch.float16,
        device_map="auto"
    )

    # 3. LoRA μ„¤μ • μ μ©
    print("\nβ™οΈ  LoRA μ„¤μ • μ μ©...")
    model = prepare_model_for_kbit_training(model)
    lora_config = setup_lora_config()
    model = get_peft_model(model, lora_config)

    model.print_trainable_parameters()

    # 4. λ°μ΄ν„° λ΅λ“
    train_dataset = load_and_prepare_data(training_data_path, tokenizer)

    # 5. ν•™μµ μ„¤μ •
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=10,
        save_strategy="epoch",
        warmup_steps=100,
        report_to="none"
    )

    # 6. Trainer μ„¤μ •
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
    )

    # 7. ν•™μµ μ‹μ‘
    print("\nπ€ νμΈνλ‹ μ‹μ‘...")
    trainer.train()

    # 8. λ¨λΈ μ €μ¥
    print(f"\nπ’Ύ λ¨λΈ μ €μ¥: {output_dir}")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)

    print("\nβ… νμΈνλ‹ μ™„λ£!")
    return model, tokenizer


def test_model(model_path: str, test_text: str):
    """νμΈνλ‹λ λ¨λΈ ν…μ¤νΈ"""
    print("\nπ§ λ¨λΈ ν…μ¤νΈ μ¤‘...")

    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        load_in_4bit=True,
        torch_dtype=torch.float16,
        device_map="auto"
    )

    prompt = f"""<|system|>
λ‹Ήμ‹ μ€ TOEFL μ¤ν”Όν‚Ή ν‰κ°€ μ „λ¬Έκ°€μ…λ‹λ‹¤.
<|user|>
λ‹¤μ ν•™μƒμ λ‹µλ³€μ„ ν‰κ°€ν•΄μ£Όμ„Έμ”:

{test_text}
<|assistant|>
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            do_sample=True
        )

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("\nν‰κ°€ κ²°κ³Ό:")
    print(result)
    return result


if __name__ == "__main__":
    # νμΈνλ‹ μ‹¤ν–‰
    # μ£Όμ: GPU ν•„μ” (Google Colab λ¬΄λ£ GPU μ‚¬μ© κ°€λ¥)

    model, tokenizer = fine_tune_model(
        model_name="meta-llama/Llama-2-7b-chat-hf",  # λλ” λ‹¤λ¥Έ λ¨λΈ
        training_data_path="training_data_huggingface.jsonl",
        output_dir="./toefl_finetuned_model"
    )

    # ν…μ¤νΈ
    test_text = """University announce new policy of energy saving plan.
In the listening conversation, woman disagree..."""

    test_model("./toefl_finetuned_model", test_text)

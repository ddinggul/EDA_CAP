# M2 MacBook Proì—ì„œ LLM íŒŒì¸íŠœë‹ ê°€ì´ë“œ

## ğŸ Apple Silicon (M2) ìµœì í™” íŒŒì¸íŠœë‹

M2 ì¹©ì˜ **Neural Engine**ê³¼ **Unified Memory**ë¥¼ í™œìš©í•˜ì—¬ GPU ì—†ì´ë„ íš¨ìœ¨ì ìœ¼ë¡œ LLMì„ íŒŒì¸íŠœë‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ“‹ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

| RAM | ê¶Œì¥ ëª¨ë¸ | í•™ìŠµ ì‹œê°„ (100 ìƒ˜í”Œ) |
|-----|----------|---------------------|
| 8GB | Phi-2 (2.7B) | 30-60ë¶„ |
| 16GB | Mistral-7B â­ | 1-2ì‹œê°„ |
| 32GB+ | Llama-2-13B | 2-4ì‹œê°„ |

**í™•ì¸ ë°©ë²•:**
```bash
# Mac ì‚¬ì–‘ í™•ì¸
system_profiler SPHardwareDataType | grep "Memory"
```

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ (5ë¶„ ì„¤ì¹˜)

### 1ë‹¨ê³„: MLX ì„¤ì¹˜
```bash
# MLX í”„ë ˆì„ì›Œí¬ (Apple Silicon ìµœì í™”)
pip install mlx mlx-lm

# ì¶”ê°€ íŒ¨í‚¤ì§€
pip install numpy transformers huggingface-hub
```

### 2ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„
```bash
# CSV â†’ JSONL ë³€í™˜
python prepare_training_data.py

# HuggingFace í˜•ì‹ ì„ íƒ (2ë²ˆ)
```

### 3ë‹¨ê³„: íŒŒì¸íŠœë‹ ì‹¤í–‰
```bash
python finetune_m2_mac.py
```

**ê·¸ê²Œ ëì…ë‹ˆë‹¤!** ğŸ‰

---

## ğŸ“Š MLX vs ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ ë¹„êµ

| í”„ë ˆì„ì›Œí¬ | M2 ìµœì í™” | ë©”ëª¨ë¦¬ íš¨ìœ¨ | ì†ë„ | ì„¤ì¹˜ ë‚œì´ë„ |
|-----------|----------|-----------|------|-----------|
| **MLX** â­ | âœ… ì™„ë²½ | âœ… ìµœê³  | âœ… ë¹ ë¦„ | â­ ì‰¬ì›€ |
| PyTorch | âš ï¸ ë¶€ë¶„ | âš ï¸ ë³´í†µ | âš ï¸ ëŠë¦¼ | â­â­ ë³´í†µ |
| TensorFlow | âŒ ì—†ìŒ | âš ï¸ ë³´í†µ | âŒ ë§¤ìš° ëŠë¦¼ | â­â­â­ ì–´ë ¤ì›€ |

---

## ğŸ¯ ê¶Œì¥ ëª¨ë¸ (MLX Community)

### Option 1: Mistral-7B (ê°€ì¥ ê¶Œì¥ â­)
```python
model = "mlx-community/Mistral-7B-Instruct-v0.2-4bit"
```
- **ì¥ì **: ë¹ ë¥´ê³  ì •í™•í•¨
- **RAM**: 16GB ì´ìƒ
- **í•™ìŠµ ì‹œê°„**: 1-2ì‹œê°„

### Option 2: Llama-2-7B (ì•ˆì •ì )
```python
model = "mlx-community/Llama-2-7b-chat-4bit"
```
- **ì¥ì **: ê²€ì¦ëœ ì„±ëŠ¥
- **RAM**: 16GB ì´ìƒ
- **í•™ìŠµ ì‹œê°„**: 1.5-2.5ì‹œê°„

### Option 3: Phi-2 (ê°€ë²¼ì›€)
```python
model = "mlx-community/Phi-2-4bit"
```
- **ì¥ì **: 8GB RAMì—ì„œ ì‘ë™
- **RAM**: 8GB ì´ìƒ
- **í•™ìŠµ ì‹œê°„**: 30-60ë¶„

### Option 4: Gemma-7B (ìµœì‹ )
```python
model = "mlx-community/gemma-7b-it-4bit"
```
- **ì¥ì **: Googleì˜ ìµœì‹  ëª¨ë¸, í•œêµ­ì–´ ì§€ì› ì¢‹ìŒ
- **RAM**: 16GB ì´ìƒ

---

## ğŸ’» ì‹¤ì „ ì‚¬ìš©ë²•

### ìë™ ëª¨ë“œ (ì¶”ì²œ)
```bash
python finetune_m2_mac.py
```
ìŠ¤í¬ë¦½íŠ¸ê°€ ìë™ìœ¼ë¡œ:
1. ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì²´í¬
2. ìµœì  ëª¨ë¸ ì¶”ì²œ
3. ë°ì´í„° ë³€í™˜
4. íŒŒì¸íŠœë‹ ì‹¤í–‰
5. ëª¨ë¸ í…ŒìŠ¤íŠ¸

### ìˆ˜ë™ ëª¨ë“œ (ê³ ê¸‰)
```python
from finetune_m2_mac import fine_tune_with_mlx, test_finetuned_model

# íŒŒì¸íŠœë‹
adapter_path = fine_tune_with_mlx(
    model_name="mlx-community/Mistral-7B-Instruct-v0.2-4bit",
    data_dir="./data",
    output_dir="./my_model",
    num_epochs=3,
    batch_size=4,
    learning_rate=1e-5
)

# í…ŒìŠ¤íŠ¸
test_finetuned_model(
    model_name="mlx-community/Mistral-7B-Instruct-v0.2-4bit",
    adapter_path=adapter_path,
    test_text="Your test text here"
)
```

---

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
batch_size=2  # ê¸°ë³¸ê°’: 4

# LoRA rank ì¤„ì´ê¸°
lora_rank=8  # ê¸°ë³¸ê°’: 16
```

### ë” ë¹ ë¥¸ í•™ìŠµ
```python
# í•™ìŠµë¥  ì¦ê°€
learning_rate=2e-5  # ê¸°ë³¸ê°’: 1e-5

# ì—í¬í¬ ê°ì†Œ
num_epochs=2  # ê¸°ë³¸ê°’: 3
```

### ë” ë†’ì€ í’ˆì§ˆ
```python
# ì—í¬í¬ ì¦ê°€
num_epochs=5

# LoRA rank ì¦ê°€
lora_rank=32
```

---

## ğŸ“ˆ í•™ìŠµ ëª¨ë‹ˆí„°ë§

### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
```bash
# Activity Monitorë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
open -a "Activity Monitor"

# í„°ë¯¸ë„ì—ì„œ í™•ì¸
top -o mem | grep Python
```

### í•™ìŠµ ì§„í–‰ ìƒí™©
```
Iteration 10: Train loss 2.456, Val loss 2.389
Iteration 20: Train loss 2.234, Val loss 2.187
...
Iteration 100: Train loss 1.456, Val loss 1.523
```

**ì†ì‹¤(loss)ì´ ê°ì†Œí•˜ë©´ í•™ìŠµì´ ì˜ ë˜ê³  ìˆëŠ” ê²ƒì…ë‹ˆë‹¤!**

---

## ğŸ§ª íŒŒì¸íŠœë‹ í›„ ì‚¬ìš©ë²•

### Python ìŠ¤í¬ë¦½íŠ¸
```python
from mlx_lm import load, generate

# ëª¨ë¸ ë¡œë“œ
model, tokenizer = load(
    "mlx-community/Mistral-7B-Instruct-v0.2-4bit",
    adapter_path="./toefl_finetuned_mlx"
)

# í‰ê°€ ì‹¤í–‰
text = "Student's answer here..."
prompt = f"ë‹¤ìŒ TOEFL ìŠ¤í”¼í‚¹ ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”: {text}"

response = generate(model, tokenizer, prompt=prompt, max_tokens=400)
print(response)
```

### ëŒ€í™”í˜• ëª¨ë“œ
```bash
python finetune_m2_mac.py
# ë§ˆì§€ë§‰ì— 'y' ì„ íƒí•˜ì—¬ ëŒ€í™”í˜• ëª¨ë“œ ì§„ì…
```

### API ì„œë²„ë¡œ ë°°í¬
```python
# api_server.py
from flask import Flask, request, jsonify
from mlx_lm import load, generate

app = Flask(__name__)

# ëª¨ë¸ ë¡œë“œ (í•œ ë²ˆë§Œ)
model, tokenizer = load(
    "mlx-community/Mistral-7B-Instruct-v0.2-4bit",
    adapter_path="./toefl_finetuned_mlx"
)

@app.route('/evaluate', methods=['POST'])
def evaluate():
    data = request.json
    text = data.get('text', '')

    prompt = f"ë‹¤ìŒ TOEFL ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”: {text}"
    response = generate(model, tokenizer, prompt=prompt, max_tokens=400)

    return jsonify({'evaluation': response})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

ì‹¤í–‰:
```bash
pip install flask
python api_server.py
```

í…ŒìŠ¤íŠ¸:
```bash
curl -X POST http://localhost:5000/evaluate \
  -H "Content-Type: application/json" \
  -d '{"text": "I like reading books because..."}'
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™” íŒ

### 1. ì–‘ìí™” (Quantization)
MLXëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 4bit ì–‘ìí™” ëª¨ë¸ ì‚¬ìš©
- **ë©”ëª¨ë¦¬ 75% ì ˆê°**
- **ì†ë„ 2-3ë°° í–¥ìƒ**

### 2. í†µí•© ë©”ëª¨ë¦¬ í™œìš©
M2ì˜ Unified MemoryëŠ” CPU-GPU ê°„ ë³µì‚¬ ë¶ˆí•„ìš”
- ìë™ìœ¼ë¡œ ìµœì í™”ë¨

### 3. Metal Performance Shaders
Appleì˜ GPU ê°€ì† ìë™ í™œì„±í™”
- ë³„ë„ ì„¤ì • ë¶ˆí•„ìš”

### 4. ë°°ì¹˜ ì²˜ë¦¬
```python
# ì—¬ëŸ¬ ë‹µë³€ í•œë²ˆì— í‰ê°€
texts = ["answer1", "answer2", "answer3"]
results = [evaluate(text) for text in texts]
```

---

## ğŸ› ë¬¸ì œ í•´ê²°

### Q1: "MLX not found" ì˜¤ë¥˜
```bash
# Rosetta ëª¨ë“œê°€ ì•„ë‹Œ ë„¤ì´í‹°ë¸Œ Python ì‚¬ìš© í™•ì¸
python --version
# Python 3.9+ í•„ìš”

# MLX ì¬ì„¤ì¹˜
pip uninstall mlx mlx-lm
pip install mlx mlx-lm
```

### Q2: ë©”ëª¨ë¦¬ ë¶€ì¡± (Out of Memory)
```python
# í•´ê²° ë°©ë²•:
1. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© (Phi-2)
2. batch_size=2ë¡œ ê°ì†Œ
3. ë‹¤ë¥¸ ì•± ì¢…ë£Œ
4. ë§¥ ì¬ë¶€íŒ…
```

### Q3: í•™ìŠµ ì†ë„ê°€ ë„ˆë¬´ ëŠë¦¼
```bash
# Activity Monitorì—ì„œ í™•ì¸:
# - "Python" í”„ë¡œì„¸ìŠ¤ê°€ CPU 800%+ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸
# - ì‚¬ìš© ì¤‘ì´ë¼ë©´ ì •ìƒ

# ë°±ê·¸ë¼ìš´ë“œ ì•± ì¢…ë£Œ
# íŠ¹íˆ Chrome, Docker ë“±
```

### Q4: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
```bash
# HuggingFace í† í° ì„¤ì •
huggingface-cli login

# ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export HF_TOKEN="your_token_here"
```

---

## ğŸ“Š ë²¤ì¹˜ë§ˆí¬ (M2 MacBook Pro 16GB)

| ëª¨ë¸ | í•™ìŠµ ì‹œê°„ (100 ìƒ˜í”Œ) | ë©”ëª¨ë¦¬ ì‚¬ìš© | ì¶”ë¡  ì†ë„ |
|------|---------------------|-----------|----------|
| Phi-2 (2.7B) | 35ë¶„ | 6GB | 50 tokens/s |
| Mistral-7B | 1.5ì‹œê°„ | 12GB | 30 tokens/s |
| Llama-2-7B | 2ì‹œê°„ | 13GB | 25 tokens/s |
| Gemma-7B | 1.8ì‹œê°„ | 12GB | 28 tokens/s |

---

## ğŸ’° ë¹„ìš© ë¹„êµ

| ë°©ë²• | ì´ˆê¸° ë¹„ìš© | ì›” ìš´ì˜ ë¹„ìš© | í•™ìŠµ ë¹„ìš© |
|------|---------|------------|----------|
| **M2 Mac (MLX)** | $0 | $0 | $0 |
| OpenAI Fine-tuning | $0 | $50-100 | $10-50 |
| GPU í´ë¼ìš°ë“œ | $0 | $100-300 | $20-100 |

**M2 Macì„ ì´ë¯¸ ë³´ìœ í–ˆë‹¤ë©´ ì™„ì „ ë¬´ë£Œì…ë‹ˆë‹¤!** ğŸ‰

---

## ğŸ“ í•™ìŠµ ìë£Œ

- [MLX ê³µì‹ ë¬¸ì„œ](https://ml-explore.github.io/mlx/)
- [MLX Community Models](https://huggingface.co/mlx-community)
- [Apple Machine Learning](https://developer.apple.com/machine-learning/)

---

## ğŸ”„ ì—…ë°ì´íŠ¸ ë° ê°œì„ 

### ìµœì‹  MLX ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸
```bash
pip install --upgrade mlx mlx-lm
```

### ìƒˆ ëª¨ë¸ íƒìƒ‰
```bash
# HuggingFace MLX community ê²€ìƒ‰
https://huggingface.co/mlx-community
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

íŒŒì¸íŠœë‹ ì „:
- [ ] M2/M3 Mac í™•ì¸
- [ ] 16GB+ RAM (ê¶Œì¥)
- [ ] MLX ì„¤ì¹˜ ì™„ë£Œ
- [ ] í•™ìŠµ ë°ì´í„° 50ê°œ ì´ìƒ ì¤€ë¹„

íŒŒì¸íŠœë‹ ì¤‘:
- [ ] Activity Monitorë¡œ ë©”ëª¨ë¦¬ í™•ì¸
- [ ] ì†ì‹¤(loss) ê°ì†Œ í™•ì¸
- [ ] ê³¼ì í•©(overfitting) ì£¼ì˜

íŒŒì¸íŠœë‹ í›„:
- [ ] í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ê²€ì¦
- [ ] ì‹¤ì œ ë‹µë³€ìœ¼ë¡œ í‰ê°€
- [ ] API ì„œë²„ ë°°í¬ (ì„ íƒ)

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. **ìŒì„± ì¸ì‹ í†µí•©**: Whisper + MLX
2. **ì›¹ ì¸í„°í˜ì´ìŠ¤**: Streamlit ì•±
3. **ìë™ í‰ê°€ ì‹œìŠ¤í…œ**: ì‹¤ì‹œê°„ í”¼ë“œë°±
4. **ë©€í‹°ëª¨ë‹¬**: ë°œìŒ ë¶„ì„ ì¶”ê°€

**M2 Macìœ¼ë¡œ ê°•ë ¥í•œ TOEFL í‰ê°€ ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ë³´ì„¸ìš”!** ğŸ¯

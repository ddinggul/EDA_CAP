# ğŸš€ M2 Mac ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ (5ë¶„)

TOEFL ìŠ¤í”¼í‚¹ í‰ê°€ LLM íŒŒì¸íŠœë‹ì„ M2 MacBook Proì—ì„œ ì‹œì‘í•˜ì„¸ìš”!

---

## âš¡ ì´ˆê°„ë‹¨ 3ë‹¨ê³„ ì‹œì‘

### 1ï¸âƒ£ ì„¤ì¹˜ (2ë¶„)
```bash
chmod +x setup_m2.sh
./setup_m2.sh
```

### 2ï¸âƒ£ ë°ì´í„° ì¤€ë¹„ (1ë¶„)
```bash
# CSV íŒŒì¼ì„ toefl_evaluations.csvë¡œ ì €ì¥
# ê·¸ ë‹¤ìŒ:
python prepare_training_data.py
# 2ë²ˆ ì„ íƒ (HuggingFace)
```

### 3ï¸âƒ£ íŒŒì¸íŠœë‹ (1-2ì‹œê°„, ìë™)
```bash
python finetune_m2_mac.py
# 1ë²ˆ ì„ íƒ (Mistral-7B)
# ì´ì œ ì»¤í”¼ í•œ ì” í•˜ì„¸ìš” â˜•
```

**ë!** ğŸ‰

---

## ğŸ’» ì‹¤í–‰ ì˜ˆì‹œ

### í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸
```python
from mlx_lm import load, generate

# ëª¨ë¸ ë¡œë“œ
model, tokenizer = load(
    "mlx-community/Mistral-7B-Instruct-v0.2-4bit",
    adapter_path="./toefl_finetuned_mlx"
)

# í‰ê°€ ì‹¤í–‰
text = "I prefer studying subjects that interest me..."
prompt = f"ë‹¤ìŒ TOEFL ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”: {text}"
result = generate(model, tokenizer, prompt=prompt, max_tokens=400)

print(result)
```

### API ì„œë²„ë¡œ ì‹¤í–‰
```bash
# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install flask flask-cors

# ì„œë²„ ì‹œì‘
python api_server_mlx.py

# ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:5000/evaluate \
  -H "Content-Type: application/json" \
  -d '{"text": "I like reading books because it helps me learn new things."}'
```

---

## ğŸ“Š ë‚´ M2 Macì—ì„œ ì–´ë–¤ ëª¨ë¸?

```bash
# RAM í™•ì¸
sysctl hw.memsize | awk '{print $2/1024/1024/1024 "GB"}'
```

| RAM | ì¶”ì²œ ëª¨ë¸ | í•™ìŠµ ì‹œê°„ |
|-----|----------|----------|
| 8GB | Phi-2 | 30-60ë¶„ |
| 16GB | **Mistral-7B** â­ | 1-2ì‹œê°„ |
| 32GB+ | Llama-2-13B | 2-4ì‹œê°„ |

---

## ğŸ¯ ì‹¤ì „ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: Python ìŠ¤í¬ë¦½íŠ¸ì— í†µí•©
```python
# my_app.py
from mlx_lm import load, generate

class TOEFLEvaluator:
    def __init__(self):
        self.model, self.tokenizer = load(
            "mlx-community/Mistral-7B-Instruct-v0.2-4bit",
            adapter_path="./toefl_finetuned_mlx"
        )

    def evaluate(self, text):
        prompt = f"TOEFL ë‹µë³€ í‰ê°€: {text}"
        return generate(self.model, self.tokenizer,
                       prompt=prompt, max_tokens=400)

# ì‚¬ìš©
evaluator = TOEFLEvaluator()
score = evaluator.evaluate("Your answer here...")
print(score)
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜
```python
# Streamlit ì•±
import streamlit as st
from mlx_lm import load, generate

st.title("ğŸ“ TOEFL ìŠ¤í”¼í‚¹ í‰ê°€")

# ëª¨ë¸ ë¡œë“œ (ìºì‹œ)
@st.cache_resource
def load_model():
    return load("mlx-community/Mistral-7B-Instruct-v0.2-4bit",
                adapter_path="./toefl_finetuned_mlx")

model, tokenizer = load_model()

# ì…ë ¥
text = st.text_area("ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”:")

if st.button("í‰ê°€í•˜ê¸°"):
    with st.spinner("í‰ê°€ ì¤‘..."):
        prompt = f"TOEFL ë‹µë³€ í‰ê°€: {text}"
        result = generate(model, tokenizer, prompt=prompt, max_tokens=400)
        st.success(result)
```

ì‹¤í–‰:
```bash
pip install streamlit
streamlit run app.py
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: REST API ì„œë²„
```bash
# ì„œë²„ ì‹œì‘
python api_server_mlx.py --port 8080

# ë‹¤ë¥¸ ì•±ì—ì„œ í˜¸ì¶œ
import requests

response = requests.post('http://localhost:8080/evaluate',
    json={'text': 'I like reading...'})
print(response.json()['evaluation'])
```

---

## ğŸ”§ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸

### Q1: í•™ìŠµ ì¤‘ ë©ˆì¶˜ ê²ƒ ê°™ì•„ìš”
**A:** Activity Monitorë¥¼ ì—´ì–´ì„œ "Python" í”„ë¡œì„¸ìŠ¤ í™•ì¸
- CPU 800%+ ì‚¬ìš© ì¤‘ì´ë©´ ì •ìƒ (í•™ìŠµ ì¤‘)
- ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°

### Q2: ì–´ë–¤ ëª¨ë¸ì´ ê°€ì¥ ì¢‹ë‚˜ìš”?
**A:** TOEFL í‰ê°€ì—ëŠ” **Mistral-7B** ì¶”ì²œ
- ë¹ ë¥´ê³  ì •í™•í•¨
- í•œêµ­ì–´/ì˜ì–´ ëª¨ë‘ ì§€ì›
- 16GB RAMì—ì„œ ì•ˆì •ì 

### Q3: ë°ì´í„°ê°€ ì ìœ¼ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
**A:** 50ê°œ ë¯¸ë§Œì´ë©´:
1. Few-shot learning ì‚¬ìš© (ì˜ˆì‹œë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨)
2. ë” ë§ì€ ì—í¬í¬ (5-7)
3. ë°ì´í„° ì¦ê°• (paraphrasing)

### Q4: ë¹„ìš©ì´ ë“œë‚˜ìš”?
**A:** **ì™„ì „ ë¬´ë£Œ!**
- ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ë¬´ë£Œ
- í•™ìŠµ: M2 Macì—ì„œ ë¬´ë£Œ
- ì¶”ë¡ : ë¬´ë£Œ
- ìš´ì˜: ë¬´ë£Œ

---

## ğŸ“ˆ ì„±ëŠ¥ ê°œì„  íŒ

### 1. ë” ë¹ ë¥¸ í•™ìŠµ
```python
# batch_size ì¦ê°€ (ë©”ëª¨ë¦¬ í—ˆìš© ì‹œ)
batch_size=8  # ê¸°ë³¸ê°’: 4

# í•™ìŠµë¥  ì¦ê°€
learning_rate=2e-5  # ê¸°ë³¸ê°’: 1e-5
```

### 2. ë” ë†’ì€ ì •í™•ë„
```python
# ì—í¬í¬ ì¦ê°€
num_epochs=5  # ê¸°ë³¸ê°’: 3

# LoRA rank ì¦ê°€
lora_rank=32  # ê¸°ë³¸ê°’: 16
```

### 3. ë©”ëª¨ë¦¬ ì ˆì•½
```python
# batch_size ê°ì†Œ
batch_size=2

# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
model = "mlx-community/Phi-2-4bit"
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **ìŒì„± ì¸ì‹ ì¶”ê°€**
   ```bash
   pip install openai-whisper
   # ìŒì„± â†’ í…ìŠ¤íŠ¸ â†’ í‰ê°€
   ```

2. **ì›¹ ì¸í„°í˜ì´ìŠ¤**
   ```bash
   pip install streamlit
   streamlit run web_app.py
   ```

3. **ë°°í¬**
   - Docker ì»¨í…Œì´ë„ˆ
   - Railway/Renderì— ë°°í¬
   - iOS ì•±ì— í†µí•© (Core ML ë³€í™˜)

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **ìƒì„¸ ê°€ì´ë“œ**: `M2_MAC_SETUP.md`
- **íŒŒì¸íŠœë‹ ì´ë¡ **: `FINETUNING_GUIDE.md`
- **MLX ë¬¸ì„œ**: https://ml-explore.github.io/mlx/
- **MLX Models**: https://huggingface.co/mlx-community

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ì‹œì‘ ì „:
- [ ] M2/M3 Mac ì¤€ë¹„
- [ ] 16GB+ RAM (ê¶Œì¥)
- [ ] CSV ë°ì´í„° 50ê°œ ì´ìƒ

ì„¤ì¹˜:
- [ ] `./setup_m2.sh` ì‹¤í–‰ ì™„ë£Œ
- [ ] MLX ì„¤ì¹˜ í™•ì¸
- [ ] í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„

íŒŒì¸íŠœë‹:
- [ ] ë°ì´í„° ë³€í™˜ ì™„ë£Œ
- [ ] ëª¨ë¸ ì„ íƒ
- [ ] í•™ìŠµ ì™„ë£Œ (1-2ì‹œê°„)
- [ ] í…ŒìŠ¤íŠ¸ í†µê³¼

ë°°í¬:
- [ ] API ì„œë²„ í…ŒìŠ¤íŠ¸
- [ ] ì‹¤ì œ ë‹µë³€ìœ¼ë¡œ ê²€ì¦
- [ ] í”„ë¡œë•ì…˜ ì¤€ë¹„

---

## ğŸŠ ì™„ë£Œ!

**ì¶•í•˜í•©ë‹ˆë‹¤!** M2 Macì—ì„œ TOEFL í‰ê°€ AIë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤!

ë¬¸ì œê°€ ìˆë‚˜ìš”? â†’ `M2_MAC_SETUP.md`ì˜ "ë¬¸ì œ í•´ê²°" ì„¹ì…˜ ì°¸ê³ 

**Happy Fine-tuning!** ğŸš€

# ğŸ¤ ìŒì„± ê¸°ë°˜ TOEFL í‰ê°€ ì‹œìŠ¤í…œ ê°€ì´ë“œ

ìŒì„± íŒŒì¼(WAV) + ëŒ€ë³¸ + í”¼ë“œë°±ì„ í™œìš©í•œ ë©€í‹°ëª¨ë‹¬ íŒŒì¸íŠœë‹

---

## ğŸ“Š ì‹œìŠ¤í…œ êµ¬ì¡°

```
[í•™ìƒ ìŒì„± WAV] â”€â”€â”¬â”€â†’ [MFCC/ìŒì„± íŠ¹ì§• ì¶”ì¶œ] â”€â†’ [LSTM ëª¨ë¸] â”€â†’ ë°œìŒ/ìœ ì°½ì„± ì ìˆ˜
                  â”‚                                              â†“
[ëŒ€ë³¸/í”¼ë“œë°±] â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ [LLM (Mistral)] â”€â†’ ì¢…í•© í‰ê°€
                                                               â†—
                                                    ë‚´ìš©/ë¬¸ë²• í‰ê°€
```

### ëª¨ë¸ êµ¬ì„±
1. **ìŒì„± ë¶„ì„ ëª¨ë¸** (LSTM)
   - ì…ë ¥: MFCC, Pitch, Energy ë“± ìŒì„± íŠ¹ì§•
   - ì¶œë ¥: ë°œìŒ ì ìˆ˜, ìœ ì°½ì„± ì ìˆ˜ (0-4ì )

2. **LLM ëª¨ë¸** (Mistral-7B)
   - ì…ë ¥: ëŒ€ë³¸ + ìŒì„± ì ìˆ˜
   - ì¶œë ¥: ë‚´ìš©/ë¬¸ë²• í‰ê°€ + ì¢…í•© í”¼ë“œë°±

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ (4ë‹¨ê³„)

### ì „ì œ ì¡°ê±´
```
ë°ì´í„° êµ¬ì¡°:
/audio/
  â”œâ”€â”€ student_1.wav
  â”œâ”€â”€ student_2.wav
  â””â”€â”€ ...
feedback.csv (íŒŒì¼ì´ë¦„, í…ìŠ¤íŠ¸, í”¼ë“œë°±, ì ìˆ˜ ì»¬ëŸ¼)
```

### 1ë‹¨ê³„: ìŒì„± íŠ¹ì§• ì¶”ì¶œ (10ë¶„)
```bash
python audio_feature_extraction.py \
  --audio_dir ./audio \
  --csv feedback.csv \
  --output audio_features.jsonl
```

**ì¶œë ¥**: `audio_features.jsonl` (MFCC, Pitch ë“± ìŒì„± íŠ¹ì§•)

### 2ë‹¨ê³„: ìŒì„± ëª¨ë¸ í•™ìŠµ (GPU ì„œë²„, 30ë¶„-1ì‹œê°„)
```bash
python train_audio_model.py \
  --data audio_features.jsonl \
  --output ./audio_model \
  --epochs 100 \
  --batch_size 32
```

**ì¶œë ¥**: `./audio_model/` (ë°œìŒ/ìœ ì°½ì„± ì˜ˆì¸¡ ëª¨ë¸)

### 3ë‹¨ê³„: LLM íŒŒì¸íŠœë‹ (GPU ì„œë²„, 1-2ì‹œê°„)
```bash
# ë°ì´í„° ì¤€ë¹„
python prepare_training_data.py

# íŒŒì¸íŠœë‹ (HuggingFace on GPU)
python finetune_huggingface.py
```

**ì¶œë ¥**: `./toefl_finetuned_model/` (ë‚´ìš©/ë¬¸ë²• í‰ê°€ LLM)

### 4ë‹¨ê³„: í†µí•© í‰ê°€ ì‹œìŠ¤í…œ ì‹¤í–‰
```bash
# ë‹¨ì¼ í‰ê°€
python integrated_evaluation_system.py \
  --audio ./audio/student_1.wav \
  --transcript "I prefer studying subjects that interest me..." \
  --audio_model ./audio_model \
  --llm_type mlx

# ì¼ê´„ í‰ê°€
python integrated_evaluation_system.py \
  --batch \
  --audio_dir ./audio \
  --csv feedback.csv
```

---

## ğŸ“ ë°ì´í„° ì¤€ë¹„

### CSV í˜•ì‹
```csv
íŒŒì¼ ì´ë¦„,í…ìŠ¤íŠ¸,í…ìŠ¤íŠ¸ í”¼ë“œë°±,ë°œìŒ,fluency,ë‚´ìš©,ë¬¸ë²•/í‘œí˜„,total_score
Q1 í•™ìƒA,"I prefer studying...",#ë°œìŒ: R/Lêµ¬ë¶„,3.0,3.5,3.2,2.8,3.1
Q2 í•™ìƒB,"University announced...",#í‘œí˜„: ì ì ˆ,3.5,3.0,3.8,3.2,3.4
```

### ìŒì„± íŒŒì¼
- **í¬ë§·**: WAV (ê¶Œì¥), MP3
- **ìƒ˜í”Œë ˆì´íŠ¸**: 16kHz (ê¶Œì¥)
- **ë¹„íŠ¸ë ˆì´íŠ¸**: 16-bit
- **ê¸¸ì´**: 30ì´ˆ - 2ë¶„

---

## ğŸ”¬ ìŒì„± íŠ¹ì§• ìƒì„¸

### 1. MFCC (Mel-frequency cepstral coefficients)
- **ìš©ë„**: ë°œìŒ ë¶„ì„
- **íŠ¹ì§•**: 13ì°¨ì› MFCC + Delta + Delta-Delta
- **ë¶„ì„**: ììŒ/ëª¨ìŒ ëª…í™•ì„±, ìŒì†Œ ì •í™•ë„

### 2. Prosody (ìš´ìœ¨)
- **Pitch (F0)**: ì–µì–‘, í†¤
- **Energy**: ìŒëŸ‰, ê°•ì¡°
- **Tempo**: ë§í•˜ê¸° ì†ë„
- **ìš©ë„**: ìœ ì°½ì„± ë¶„ì„

### 3. Fluency (ìœ ì°½ì„±)
- **Pause Detection**: íœ´ì§€ ë¹ˆë„/ê¸¸ì´
- **Speech Rate**: ë°œí™” ì†ë„
- **Articulation Rate**: ìŒì ˆ ì†ë„
- **ìš©ë„**: ë§ë”ë“¬, ìì—°ìŠ¤ëŸ¬ì›€

### 4. Pronunciation (ë°œìŒ)
- **Spectral Contrast**: ììŒ ëª…í™•ì„±
- **Chroma**: ìŒë†’ì´ ì •í™•ë„
- **Formants**: ëª¨ìŒ ë¶„ì„
- **ìš©ë„**: R/L êµ¬ë¶„, ì¥ë‹¨ëª¨ìŒ

---

## ğŸ¯ ëª¨ë¸ í•™ìŠµ ì „ëµ

### Option A: ë³„ë„ í•™ìŠµ (ê¶Œì¥ â­)
```
1. ìŒì„± ëª¨ë¸ (LSTM) í•™ìŠµ â†’ ë°œìŒ/ìœ ì°½ì„±
2. LLM íŒŒì¸íŠœë‹ â†’ ë‚´ìš©/ë¬¸ë²•
3. ì¶”ë¡  ì‹œ ê²°í•©
```

**ì¥ì **:
- ê° ëª¨ë¸ ë…ë¦½ì  ê°œì„ 
- í•™ìŠµ ê°„ë‹¨
- ë””ë²„ê¹… ìš©ì´

### Option B: End-to-End í•™ìŠµ
```
ìŒì„± íŠ¹ì§• â†’ [Audio Encoder] â†’ [LLM] â†’ ì¢…í•© í‰ê°€
```

**ì¥ì **:
- ì™„ì „ í†µí•©
- ë” ë‚˜ì€ ì„±ëŠ¥ (ì´ë¡ ì )

**ë‹¨ì **:
- ë³µì¡í•¨
- ë°ì´í„° ë§ì´ í•„ìš” (1000+)

---

## ğŸ’» GPU ì„œë²„ í•™ìŠµ ê°€ì´ë“œ

### í´ë¼ìš°ë“œ GPU ì˜µì…˜
1. **Google Colab** (ë¬´ë£Œ/Pro)
   - ë¬´ë£Œ: T4 GPU (ì œí•œì )
   - Pro ($10/ì›”): V100, ë” ê¸´ ì‹œê°„

2. **Paperspace Gradient** (ì €ë ´)
   - P5000 GPU: $0.51/ì‹œê°„
   - RTX 4000: $0.56/ì‹œê°„

3. **RunPod** (ê°€ì¥ ì €ë ´)
   - RTX 3090: $0.34/ì‹œê°„
   - A100: $1.39/ì‹œê°„

4. **Lambda Labs**
   - A100: $1.10/ì‹œê°„

### Google Colabì—ì„œ ì‹¤í–‰

```python
# Colab ë…¸íŠ¸ë¶
!git clone https://github.com/your-repo
%cd EDA_CAP

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
!pip install -r requirements_finetuning.txt

# ë°ì´í„° ì—…ë¡œë“œ (Google Drive)
from google.colab import drive
drive.mount('/content/drive')

# 1. ìŒì„± íŠ¹ì§• ì¶”ì¶œ
!python audio_feature_extraction.py \
  --audio_dir /content/drive/MyDrive/audio \
  --csv /content/drive/MyDrive/feedback.csv

# 2. ìŒì„± ëª¨ë¸ í•™ìŠµ
!python train_audio_model.py \
  --data audio_features.jsonl \
  --epochs 100

# 3. LLM íŒŒì¸íŠœë‹
!python finetune_huggingface.py

# ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
!zip -r models.zip audio_model toefl_finetuned_model
```

---

## ğŸ§ª í‰ê°€ ë° í…ŒìŠ¤íŠ¸

### ë‹¨ì¼ í‰ê°€
```python
from integrated_evaluation_system import IntegratedTOEFLEvaluator

# ì‹œìŠ¤í…œ ì´ˆê¸°í™”
evaluator = IntegratedTOEFLEvaluator(
    audio_model_dir="./audio_model",
    llm_type="mlx",
    llm_model="mlx-community/Mistral-7B-Instruct-v0.2-4bit",
    llm_adapter_path="./toefl_finetuned_mlx"
)

# í‰ê°€ ì‹¤í–‰
result = evaluator.evaluate_complete(
    audio_path="student_1.wav",
    transcript="I prefer studying subjects that interest me..."
)

print(f"ë°œìŒ: {result['pronunciation_score']:.2f}/4.0")
print(f"ìœ ì°½ì„±: {result['fluency_score']:.2f}/4.0")
print(result['content_evaluation'])
```

### ì¼ê´„ í‰ê°€
```python
results = evaluator.batch_evaluate(
    audio_dir="./audio",
    csv_path="feedback.csv",
    output_path="results.jsonl"
)
```

---

## ğŸ“Š ì„±ëŠ¥ ë¶„ì„

### ìŒì„± ëª¨ë¸ í‰ê°€
```python
import json
import numpy as np

# ì˜ˆì¸¡ vs ì‹¤ì œ
with open('audio_features.jsonl', 'r') as f:
    data = [json.loads(line) for line in f]

predictions = []
ground_truth = []

for item in data:
    # ì˜ˆì¸¡ ì‹¤í–‰
    pred = predict_audio_scores(item['audio_features'])
    predictions.append([pred['pronunciation'], pred['fluency']])

    # ì‹¤ì œ ì ìˆ˜
    gt = item['ground_truth']
    ground_truth.append([gt['pronunciation_score'], gt['fluency_score']])

# RMSE ê³„ì‚°
rmse = np.sqrt(np.mean((np.array(predictions) - np.array(ground_truth))**2))
print(f"RMSE: {rmse:.3f}")
```

### LLM í‰ê°€
- Human evaluation (ì‚¬ëŒì´ ì§ì ‘ í‰ê°€)
- BLEU score (í”¼ë“œë°± í…ìŠ¤íŠ¸ ìœ ì‚¬ë„)
- Correlation with human scores (ì ìˆ˜ ìƒê´€ê´€ê³„)

---

## ğŸ“ í•™ìŠµ íŒ

### ë°ì´í„° ì¦ê°•
```python
# ìŒì„± ë°ì´í„° ì¦ê°•
import librosa
import numpy as np

def augment_audio(audio_path):
    y, sr = librosa.load(audio_path)

    # 1. ì†ë„ ë³€ê²½ (0.9x - 1.1x)
    y_fast = librosa.effects.time_stretch(y, rate=1.1)
    y_slow = librosa.effects.time_stretch(y, rate=0.9)

    # 2. í”¼ì¹˜ ë³€ê²½ (+/- 2 semitones)
    y_high = librosa.effects.pitch_shift(y, sr=sr, n_steps=2)
    y_low = librosa.effects.pitch_shift(y, sr=sr, n_steps=-2)

    # 3. ë…¸ì´ì¦ˆ ì¶”ê°€
    noise = np.random.randn(len(y)) * 0.005
    y_noise = y + noise

    return [y, y_fast, y_slow, y_high, y_low, y_noise]
```

### Transfer Learning
```python
# ì‚¬ì „í•™ìŠµ ëª¨ë¸ í™œìš©
# 1. Wav2Vec2 (ìŒì„± íŠ¹ì§•)
from transformers import Wav2Vec2Processor, Wav2Vec2Model

processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base")
model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base")

# 2. Whisper (ìŒì„± ì¸ì‹)
import whisper
model = whisper.load_model("base")
result = model.transcribe("audio.wav")
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### Q1: ìŒì„± íŠ¹ì§• ì¶”ì¶œì´ ë„ˆë¬´ ëŠë¦¼
**A**: ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš©
```python
from multiprocessing import Pool

def process_file(audio_path):
    return extractor.extract_all_features(audio_path)

with Pool(8) as p:
    results = p.map(process_file, audio_files)
```

### Q2: LSTM ëª¨ë¸ ê³¼ì í•©
**A**:
- Dropout ì¦ê°€ (0.3 â†’ 0.5)
- L2 regularization ì¶”ê°€
- ë°ì´í„° ì¦ê°•
- Early stopping

### Q3: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
**A**:
- Batch size ê°ì†Œ (32 â†’ 16)
- Gradient accumulation ì‚¬ìš©
- Mixed precision training (fp16)

---

## ğŸ“¦ í•„ìš” íŒ¨í‚¤ì§€

```bash
# ìŒì„± ì²˜ë¦¬
pip install librosa soundfile

# ë”¥ëŸ¬ë‹
pip install torch torchvision torchaudio
pip install transformers datasets

# ìŒì„± ì¸ì‹ (ì„ íƒ)
pip install openai-whisper

# MLX (M2 Mac)
pip install mlx mlx-lm

# ê¸°íƒ€
pip install pandas numpy scikit-learn
```

---

## ğŸ¯ ì‹¤ì „ ì›Œí¬í”Œë¡œìš°

### ê°œë°œ ë‹¨ê³„
```bash
# 1. ì†Œê·œëª¨ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ (10-20ê°œ)
python audio_feature_extraction.py --audio_dir ./sample --csv sample.csv

# 2. ìŒì„± ëª¨ë¸ í”„ë¡œí† íƒ€ì… (ì ì€ epochs)
python train_audio_model.py --data sample_features.jsonl --epochs 20

# 3. í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
python integrated_evaluation_system.py --audio sample.wav --transcript "..."
```

### í”„ë¡œë•ì…˜ ë‹¨ê³„
```bash
# 1. ì „ì²´ ë°ì´í„° íŠ¹ì§• ì¶”ì¶œ
python audio_feature_extraction.py --audio_dir ./all_audio --csv all_feedback.csv

# 2. GPU ì„œë²„ì—ì„œ ìŒì„± ëª¨ë¸ í•™ìŠµ
python train_audio_model.py --data audio_features.jsonl --epochs 100

# 3. GPU ì„œë²„ì—ì„œ LLM íŒŒì¸íŠœë‹
python finetune_huggingface.py

# 4. M2 Macìœ¼ë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë°°í¬
python integrated_evaluation_system.py --batch --audio_dir ./test --csv test.csv
```

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥

### ë°ì´í„° ê·œëª¨ë³„ ì„±ëŠ¥
| ë°ì´í„° ìˆ˜ | ìŒì„± ëª¨ë¸ RMSE | LLM í’ˆì§ˆ | í•™ìŠµ ì‹œê°„ |
|----------|--------------|---------|----------|
| 50ê°œ | 0.8-1.0 | â­â­â­ | 30ë¶„ |
| 200ê°œ | 0.5-0.7 | â­â­â­â­ | 1-2ì‹œê°„ |
| 500ê°œ+ | 0.3-0.5 | â­â­â­â­â­ | 3-4ì‹œê°„ |

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ì¤€ë¹„:
- [ ] WAV íŒŒì¼ 50ê°œ ì´ìƒ
- [ ] CSV íŒŒì¼ (ëŒ€ë³¸ + í”¼ë“œë°±)
- [ ] GPU ì„œë²„ ì¤€ë¹„ (Colab/RunPod ë“±)

í•™ìŠµ:
- [ ] ìŒì„± íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ
- [ ] ìŒì„± ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (RMSE < 0.7)
- [ ] LLM íŒŒì¸íŠœë‹ ì™„ë£Œ
- [ ] í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸

ë°°í¬:
- [ ] M2 Macì—ì„œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
- [ ] API ì„œë²„ êµ¬ì¶• (ì„ íƒ)
- [ ] ì›¹ ì¸í„°í˜ì´ìŠ¤ (ì„ íƒ)

---

**ìŒì„± ê¸°ë°˜ í‰ê°€ë¡œ ë” ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ TOEFL ì‹œìŠ¤í…œì„ ë§Œë“œì„¸ìš”!** ğŸ¤ğŸ“

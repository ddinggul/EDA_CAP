"""
M2 MacBook Pro ìµœì í™” LLM íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
MLX í”„ë ˆì„ì›Œí¬ ì‚¬ìš© (Apple Silicon ìµœì í™”)

ì§€ì› ëª¨ë¸:
- Llama-2-7B/13B
- Mistral-7B
- Phi-2/3
- Gemma-7B
"""

import json
import time
from pathlib import Path
from typing import Optional

try:
    import mlx.core as mx
    import mlx.nn as nn
    import mlx.optimizers as optim
    from mlx_lm import load, generate
    from mlx_lm.tuner import datasets, utils
    from mlx_lm.tuner.trainer import TrainingArgs, train
    MLX_AVAILABLE = True
except ImportError:
    MLX_AVAILABLE = False
    print("âš ï¸  MLXê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜: pip install mlx mlx-lm")


def prepare_data_for_mlx(jsonl_path: str, output_dir: str = "./data"):
    """
    JSONL ë°ì´í„°ë¥¼ MLX í˜•ì‹ìœ¼ë¡œ ë³€í™˜

    Args:
        jsonl_path: ì…ë ¥ JSONL íŒŒì¼ (prepare_training_data.pyë¡œ ìƒì„±)
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    """
    Path(output_dir).mkdir(exist_ok=True)

    print(f"ğŸ“‚ ë°ì´í„° ë³€í™˜ ì¤‘: {jsonl_path}")

    with open(jsonl_path, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]

    # MLX í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (train/valid ë¶„í• )
    train_size = int(len(data) * 0.9)
    train_data = data[:train_size]
    valid_data = data[train_size:]

    # train.jsonl ì €ì¥
    with open(f"{output_dir}/train.jsonl", 'w', encoding='utf-8') as f:
        for item in train_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    # valid.jsonl ì €ì¥
    with open(f"{output_dir}/valid.jsonl", 'w', encoding='utf-8') as f:
        for item in valid_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    print(f"âœ… í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ")
    print(f"âœ… ê²€ì¦ ë°ì´í„°: {len(valid_data)}ê°œ")
    print(f"âœ… ì €ì¥ ìœ„ì¹˜: {output_dir}/")

    return train_data, valid_data


def check_m2_memory():
    """M2 Mac ë©”ëª¨ë¦¬ í™•ì¸"""
    import psutil

    total_ram = psutil.virtual_memory().total / (1024**3)
    available_ram = psutil.virtual_memory().available / (1024**3)

    print(f"\nğŸ’» ì‹œìŠ¤í…œ ì •ë³´:")
    print(f"ì´ RAM: {total_ram:.1f} GB")
    print(f"ì‚¬ìš© ê°€ëŠ¥ RAM: {available_ram:.1f} GB")

    # M2 Pro/Max/Ultra í™•ì¸
    import platform
    print(f"ì¹©ì…‹: {platform.processor()}")

    if total_ram >= 16:
        print("âœ… 7B ëª¨ë¸ íŒŒì¸íŠœë‹ ê°€ëŠ¥")
        if total_ram >= 32:
            print("âœ… 13B ëª¨ë¸ íŒŒì¸íŠœë‹ ê°€ëŠ¥")
    else:
        print("âš ï¸  RAM ë¶€ì¡±. ë” ì‘ì€ ëª¨ë¸ ê¶Œì¥ (Phi-2)")

    return total_ram


def fine_tune_with_mlx(
    model_name: str = "mlx-community/Mistral-7B-Instruct-v0.2-4bit",
    data_dir: str = "./data",
    output_dir: str = "./toefl_finetuned_mlx",
    num_epochs: int = 3,
    batch_size: int = 4,
    learning_rate: float = 1e-5,
    lora_rank: int = 16
):
    """
    MLXë¥¼ ì‚¬ìš©í•œ M2 Mac ìµœì í™” íŒŒì¸íŠœë‹

    Args:
        model_name: HuggingFace ëª¨ë¸ ë˜ëŠ” MLX community ëª¨ë¸
        data_dir: í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬
        output_dir: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        num_epochs: í•™ìŠµ ì—í¬í¬
        batch_size: ë°°ì¹˜ í¬ê¸°
        learning_rate: í•™ìŠµë¥ 
        lora_rank: LoRA rank (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
    """

    if not MLX_AVAILABLE:
        print("âŒ MLXë¥¼ ë¨¼ì € ì„¤ì¹˜í•˜ì„¸ìš”: pip install mlx mlx-lm")
        return

    print("=" * 60)
    print(f"M2 Mac íŒŒì¸íŠœë‹ ì‹œì‘: {model_name}")
    print("=" * 60)

    # ë©”ëª¨ë¦¬ ì²´í¬
    total_ram = check_m2_memory()

    # í•™ìŠµ ì„¤ì •
    training_args = TrainingArgs(
        model=model_name,
        data=data_dir,
        train=True,
        iters=num_epochs * 100,  # ë°˜ë³µ íšŸìˆ˜
        batch_size=batch_size,
        learning_rate=learning_rate,
        lora_layers=16,  # LoRAë¥¼ ì ìš©í•  ë ˆì´ì–´ ìˆ˜
        adapter_file=f"{output_dir}/adapters.npz",
        save_every=100,
        test=True,
        test_batches=10
    )

    print(f"\nâš™ï¸  í•™ìŠµ ì„¤ì •:")
    print(f"  - ëª¨ë¸: {model_name}")
    print(f"  - ì—í¬í¬: {num_epochs}")
    print(f"  - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"  - í•™ìŠµë¥ : {learning_rate}")
    print(f"  - LoRA rank: {lora_rank}")

    # íŒŒì¸íŠœë‹ ì‹œì‘
    print(f"\nğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘...")
    start_time = time.time()

    try:
        # MLX íŒŒì¸íŠœë‹ ì‹¤í–‰
        train(
            model=model_name,
            data=data_dir,
            adapter_path=output_dir,
            iters=training_args.iters,
            batch_size=batch_size,
            learning_rate=learning_rate,
            save_every=100,
            test=True
        )

        elapsed = time.time() - start_time
        print(f"\nâœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„)")
        print(f"ğŸ’¾ ì–´ëŒ‘í„° ì €ì¥ ìœ„ì¹˜: {output_dir}/adapters.npz")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("MLX ë²„ì „ê³¼ ëª¨ë¸ í˜¸í™˜ì„±ì„ í™•ì¸í•˜ì„¸ìš”.")

    return output_dir


def test_finetuned_model(
    model_name: str,
    adapter_path: str,
    test_text: str,
    max_tokens: int = 500
):
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""

    print("\nğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")

    # ëª¨ë¸ ë° ì–´ëŒ‘í„° ë¡œë“œ
    print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {model_name}")
    print(f"ğŸ“¥ ì–´ëŒ‘í„° ë¡œë”©: {adapter_path}")

    model, tokenizer = load(model_name, adapter_path=adapter_path)

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""<|system|>
ë‹¹ì‹ ì€ TOEFL ìŠ¤í”¼í‚¹ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•™ìƒì˜ ë‹µë³€ì„ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:
1. ë°œìŒ (Pronunciation)
2. ìœ ì°½ì„± (Fluency)
3. ë‚´ìš© (Content)
4. ë¬¸ë²•/í‘œí˜„ (Grammar & Expression)
<|user|>
ë‹¤ìŒ í•™ìƒì˜ ë‹µë³€ì„ í‰ê°€í•´ì£¼ì„¸ìš”:

{test_text}
<|assistant|>
"""

    print("\nìƒì„± ì¤‘...")
    response = generate(
        model,
        tokenizer,
        prompt=prompt,
        max_tokens=max_tokens,
        temp=0.7
    )

    print("\n" + "=" * 60)
    print("í‰ê°€ ê²°ê³¼:")
    print("=" * 60)
    print(response)
    print("=" * 60)

    return response


def interactive_evaluation(model_name: str, adapter_path: str):
    """ëŒ€í™”í˜• í‰ê°€ ëª¨ë“œ"""

    print("\n" + "=" * 60)
    print("ğŸ“ TOEFL ìŠ¤í”¼í‚¹ í‰ê°€ ì‹œìŠ¤í…œ (MLX on M2)")
    print("=" * 60)
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥")
    print()

    model, tokenizer = load(model_name, adapter_path=adapter_path)

    while True:
        text = input("\ní•™ìƒ ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”:\n> ")

        if text.lower() in ['quit', 'exit', 'q']:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if not text.strip():
            continue

        prompt = f"""<|system|>
ë‹¹ì‹ ì€ TOEFL ìŠ¤í”¼í‚¹ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
<|user|>
ë‹¤ìŒ ë‹µë³€ì„ í‰ê°€í•´ì£¼ì„¸ìš”:

{text}
<|assistant|>
"""

        print("\ní‰ê°€ ì¤‘...")
        response = generate(model, tokenizer, prompt=prompt, max_tokens=400, temp=0.7)

        print("\n" + "â”€" * 60)
        print(response)
        print("â”€" * 60)


# M2 Macì— ìµœì í™”ëœ ê¶Œì¥ ëª¨ë¸ ëª©ë¡
M2_RECOMMENDED_MODELS = {
    "8GB RAM": [
        "mlx-community/Phi-2-4bit",  # ê°€ì¥ ê°€ë²¼ì›€
        "mlx-community/TinyLlama-1.1B-4bit",
    ],
    "16GB RAM": [
        "mlx-community/Mistral-7B-Instruct-v0.2-4bit",  # ê¶Œì¥ â­
        "mlx-community/Llama-2-7b-chat-4bit",
        "mlx-community/gemma-7b-it-4bit",
    ],
    "32GB+ RAM": [
        "mlx-community/Llama-2-13b-chat-4bit",
        "mlx-community/Mixtral-8x7B-Instruct-v0.1-4bit",
    ]
}


def print_recommended_models():
    """RAM ìš©ëŸ‰ë³„ ê¶Œì¥ ëª¨ë¸ ì¶œë ¥"""
    total_ram = check_m2_memory()

    print("\nğŸ“‹ RAM ìš©ëŸ‰ë³„ ê¶Œì¥ ëª¨ë¸:\n")

    for ram_size, models in M2_RECOMMENDED_MODELS.items():
        print(f"[{ram_size}]")
        for model in models:
            print(f"  - {model}")
        print()

    # í˜„ì¬ ì‹œìŠ¤í…œ ê¶Œì¥
    if total_ram >= 32:
        recommended = M2_RECOMMENDED_MODELS["32GB+ RAM"]
        category = "32GB+ RAM"
    elif total_ram >= 16:
        recommended = M2_RECOMMENDED_MODELS["16GB RAM"]
        category = "16GB RAM"
    else:
        recommended = M2_RECOMMENDED_MODELS["8GB RAM"]
        category = "8GB RAM"

    print(f"ğŸ’¡ í˜„ì¬ ì‹œìŠ¤í…œ({total_ram:.0f}GB)ì— ê¶Œì¥: [{category}]")
    for model in recommended:
        print(f"  âœ… {model}")


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ M2 MacBook Pro TOEFL í‰ê°€ íŒŒì¸íŠœë‹")
    print("=" * 60)

    # 1. ì‹œìŠ¤í…œ ì²´í¬ ë° ê¶Œì¥ ëª¨ë¸ ì¶œë ¥
    print_recommended_models()

    # 2. ë°ì´í„° ì¤€ë¹„
    print("\n" + "=" * 60)
    print("1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„")
    print("=" * 60)

    jsonl_file = "training_data_huggingface.jsonl"

    if not Path(jsonl_file).exists():
        print(f"âš ï¸  {jsonl_file} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € prepare_training_data.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
        print("  python prepare_training_data.py")
        exit(1)

    # JSONL â†’ MLX í˜•ì‹ ë³€í™˜
    train_data, valid_data = prepare_data_for_mlx(jsonl_file, output_dir="./data")

    # 3. ëª¨ë¸ ì„ íƒ
    print("\n" + "=" * 60)
    print("2ë‹¨ê³„: ëª¨ë¸ ì„ íƒ")
    print("=" * 60)

    print("\nê¶Œì¥ ëª¨ë¸:")
    print("1. Mistral-7B-Instruct (ê¶Œì¥, ê· í˜•ì¡íŒ ì„±ëŠ¥)")
    print("2. Llama-2-7b-chat (ì•ˆì •ì )")
    print("3. Phi-2 (ê°€ë²¼ì›€, 8GB RAM)")
    print("4. ì§ì ‘ ì…ë ¥")

    choice = input("\nì„ íƒ (1-4): ").strip()

    model_map = {
        "1": "mlx-community/Mistral-7B-Instruct-v0.2-4bit",
        "2": "mlx-community/Llama-2-7b-chat-4bit",
        "3": "mlx-community/Phi-2-4bit",
    }

    if choice in model_map:
        selected_model = model_map[choice]
    elif choice == "4":
        selected_model = input("ëª¨ë¸ ì´ë¦„ ì…ë ¥: ").strip()
    else:
        selected_model = model_map["1"]  # ê¸°ë³¸ê°’

    print(f"\nâœ… ì„ íƒëœ ëª¨ë¸: {selected_model}")

    # 4. íŒŒì¸íŠœë‹ ì‹¤í–‰
    print("\n" + "=" * 60)
    print("3ë‹¨ê³„: íŒŒì¸íŠœë‹")
    print("=" * 60)

    adapter_path = fine_tune_with_mlx(
        model_name=selected_model,
        data_dir="./data",
        output_dir="./toefl_finetuned_mlx",
        num_epochs=3,
        batch_size=4,
        learning_rate=1e-5,
        lora_rank=16
    )

    # 5. í…ŒìŠ¤íŠ¸
    print("\n" + "=" * 60)
    print("4ë‹¨ê³„: ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    test_text = """From my perspective, I prefer to study subjects that interest them.
That's because I think that it is important to find a job that is suitable for my interests."""

    test_finetuned_model(
        model_name=selected_model,
        adapter_path=adapter_path,
        test_text=test_text
    )

    # 6. ëŒ€í™”í˜• ëª¨ë“œ (ì„ íƒ)
    print("\nëŒ€í™”í˜• í‰ê°€ ëª¨ë“œë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ", end="")
    if input().lower() == 'y':
        interactive_evaluation(selected_model, adapter_path)

    print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print(f"ğŸ’¾ ì–´ëŒ‘í„° ìœ„ì¹˜: {adapter_path}/adapters.npz")
